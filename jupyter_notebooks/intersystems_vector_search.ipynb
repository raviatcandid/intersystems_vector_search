{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac3818c4",
   "metadata": {},
   "source": [
    "# Start IRIS intersystems vector database ( In terminal )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2abe34d1",
   "metadata": {},
   "source": [
    "# Docker Command to shutdown eny existing container (Run it in seperate terminal)\n",
    "docker rm iris-comm"
   ]
  },
  {
   "cell_type": "raw",
   "id": "678d07fd",
   "metadata": {},
   "source": [
    "# Docker Command to start Intersystems IRIS DB\n",
    "docker run --name iris-comm -p 1972:1972 -p 52773:52773 -e IRIS_PASSWORD=demo -e IRIS_USERNAME=demo intersystemsdc/iris-community:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a839739",
   "metadata": {},
   "source": [
    "# Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04cdfcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PERSONA = \"\"\"\n",
    "You are a Healthcare Customer Support Assistant. Your role is to assist users with inquiries strictly related to Healthcare, based on the context provided. Do not answer queries outside of context provided.\n",
    "Guidelines:\n",
    "1. Provide factually correct responses in natural language. Do not give details you don't have.\n",
    "2. Keep answers very brief and relevant to the question.\n",
    "3. Always provide complete answers and avoid asking follow-up questions.\n",
    "4. If a question is unclear and you cannot formulate a proper response, reply formally e.g., \"Sorry, I can't understand. Can you please rephrase it?\"\n",
    "5. Never request personal information from users.\n",
    "6. Respond in English ONLY\"\n",
    "7. Do Not answer questions outside the scope of provided context.\n",
    "\"\"\"\n",
    "OPENAI_LLM = \"gpt-4-turbo\"\n",
    "ASSISTANT_RESPONSE = 'Content Understood! Go ahead and ask questions.'\n",
    "\n",
    "USERNAME = 'demo'\n",
    "PASSWORD = 'demo'\n",
    "HOSTNAME = 'localhost'\n",
    "NAMESPACE = 'USER'\n",
    "PORT = '1972'\n",
    "TABLE_NAME = 'intersystems_table'\n",
    "CONNECTION_STRING = f\"iris://{USERNAME}:{PASSWORD}@{HOSTNAME}:{PORT}/{NAMESPACE}\"\n",
    "OPENAI_API_KEY = '<you-openai-api-key>'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d5f1db",
   "metadata": {},
   "source": [
    "# Database operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b52e5a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "def create_database_engine():\n",
    "    \"\"\"\n",
    "    Creates and returns a SQLAlchemy engine using a connection string from constants.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        connection_string = CONNECTION_STRING\n",
    "        engine = create_engine(connection_string)\n",
    "        return engine\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create database engine: {e}\")\n",
    "        return None\n",
    "\n",
    "def remove_table():\n",
    "    \"\"\"\n",
    "    Drops a specified table if it exists.\n",
    "    \"\"\"\n",
    "    engine = create_database_engine()\n",
    "    if engine:\n",
    "        try:\n",
    "            with engine.connect() as conn:\n",
    "                with conn.begin():\n",
    "                    drop_sql = f\"DROP TABLE IF EXISTS {TABLE_NAME}\"\n",
    "                    conn.execute(text(drop_sql))\n",
    "            return True\n",
    "        except SQLAlchemyError as e:\n",
    "            print(f\"Failed to drop table: {e}\")\n",
    "            return False\n",
    "    return False\n",
    "\n",
    "def create_data_table():\n",
    "    \"\"\"\n",
    "    Creates a new table with specified schema.\n",
    "    \"\"\"\n",
    "    engine = create_database_engine()\n",
    "    if engine:\n",
    "        try:\n",
    "            with engine.connect() as conn:\n",
    "                with conn.begin():\n",
    "                    create_sql = f\"CREATE TABLE {TABLE_NAME} (text VARCHAR(1200), text_vector VECTOR(DOUBLE, 384))\"\n",
    "                    conn.execute(text(create_sql))\n",
    "            return True\n",
    "        except SQLAlchemyError as e:\n",
    "            print(f\"Failed to create table: {e}\")\n",
    "            return False\n",
    "    return False\n",
    "\n",
    "def verify_table_existence():\n",
    "    \"\"\"\n",
    "    Checks if the specified table exists in the database.\n",
    "    \"\"\"\n",
    "    engine = create_database_engine()\n",
    "    if engine:\n",
    "        try:\n",
    "            with engine.connect() as conn:\n",
    "                with conn.begin():\n",
    "                    check_sql = f\"SELECT 1 FROM {TABLE_NAME} WHERE 1=0\"\n",
    "                    conn.execute(text(check_sql))\n",
    "            return True\n",
    "        except SQLAlchemyError as e:\n",
    "            print(f\"Table check failed - Table does not exist: {e}\")\n",
    "            return False\n",
    "    return False\n",
    "\n",
    "def insert_table_data(df):\n",
    "    \"\"\"\n",
    "    Inserts data from a DataFrame into the specified table.\n",
    "    \"\"\"\n",
    "    engine = create_database_engine()\n",
    "    if engine:\n",
    "        try:\n",
    "            with engine.connect() as conn:\n",
    "                with conn.begin():\n",
    "                    for index, row in df.iterrows():\n",
    "                        insert_sql = text(\"\"\"\n",
    "                                INSERT INTO {table_name}\n",
    "                                (text, text_vector)\n",
    "                                VALUES (:text, TO_VECTOR(:text_vector))\n",
    "                            \"\"\".format(table_name=TABLE_NAME))\n",
    "                        conn.execute(insert_sql, {\n",
    "                            'text': row['text'],\n",
    "                            'text_vector': str(row['text_vector'])\n",
    "                        })\n",
    "            return True\n",
    "        except SQLAlchemyError as e:\n",
    "            print(f\"Data insertion failed: {e}\")\n",
    "            return False\n",
    "    return False\n",
    "\n",
    "def perform_vector_search(question_embedding):\n",
    "    \"\"\"\n",
    "    Searches the database table using vector search with the given question embedding.\n",
    "    \"\"\"\n",
    "    engine = create_database_engine()\n",
    "    if engine:\n",
    "        try:\n",
    "            search_sql = text(\"\"\"\n",
    "                SELECT TOP 10 text FROM {table_name}\n",
    "                ORDER BY VECTOR_DOT_PRODUCT(text_vector, TO_VECTOR(:search_vector)) DESC\n",
    "            \"\"\".format(table_name=TABLE_NAME))\n",
    "            with engine.connect() as conn:\n",
    "                with conn.begin():\n",
    "                    results = conn.execute(search_sql, {'search_vector': str(question_embedding)}).fetchall()\n",
    "                    return [result[0] for result in results]\n",
    "        except SQLAlchemyError as e:\n",
    "            print(f\"Vector search failed: {e}\")\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "def count_table_records():\n",
    "    \"\"\"\n",
    "    Returns the count of records in the specified table.\n",
    "    \"\"\"\n",
    "    engine = create_database_engine()\n",
    "    if engine:\n",
    "        with engine.connect() as conn:\n",
    "            result = conn.execute(text(f\"SELECT COUNT(*) FROM {TABLE_NAME}\"))\n",
    "            count = result.fetchone()[0]\n",
    "            return count\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26bd0c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table exists.\n"
     ]
    }
   ],
   "source": [
    "if verify_table_existence():\n",
    "    print(\"Table exists.\")\n",
    "else:\n",
    "    print(\"Table does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa3ab401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to remove existing table: True\n",
      "Attempting to create table: True\n",
      "Verify table creation: True\n",
      "Record count in table: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Attempting to remove existing table:\", remove_table())\n",
    "print(\"Attempting to create table:\", create_data_table())\n",
    "print(\"Verify table creation:\", verify_table_existence())\n",
    "print(\"Record count in table:\", count_table_records())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494d2e84",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c52685d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ritik/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import re\n",
    "from io import BytesIO\n",
    "import PyPDF2\n",
    "\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def extract_text_from_file_path(file_path):\n",
    "    \"\"\"\n",
    "    Extract text from the provided file path.\n",
    "    Supports 'text/plain' and 'pdf' file types.\n",
    "    \"\"\"\n",
    "    if file_path.endswith('.txt'):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            return content\n",
    "\n",
    "    elif file_path.endswith('.pdf'):\n",
    "        with open(file_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            accumulated_text = ''\n",
    "            for page in pdf_reader.pages:\n",
    "                accumulated_text += page.extract_text() + '\\n'\n",
    "            return accumulated_text\n",
    "\n",
    "    else:\n",
    "        print(\"Unsupported file format. Acceptable formats include .txt and .pdf.\")\n",
    "        return None\n",
    "\n",
    "def extract_text_from_file(file_stream):\n",
    "    \"\"\"\n",
    "    Extract text from the provided file stream.\n",
    "    Supports 'text/plain' and 'pdf' file types.\n",
    "    \"\"\"\n",
    "    file_type = file_stream.type\n",
    "    if 'text/plain' in file_type:\n",
    "        file_stream.seek(0)\n",
    "        content = file_stream.read().decode('utf-8')\n",
    "        return content\n",
    "\n",
    "    elif 'pdf' in file_type:\n",
    "        file_stream.seek(0)\n",
    "        pdf_reader = PyPDF2.PdfReader(file_stream)\n",
    "        accumulated_text = ''\n",
    "        for page in pdf_reader.pages:\n",
    "            accumulated_text += page.extract_text() + '\\n'\n",
    "        return accumulated_text\n",
    "\n",
    "    else:\n",
    "        print(\"Unsupported file format. Acceptable formats include .txt and .pdf.\")\n",
    "        return None\n",
    "\n",
    "def segment_text(file_path):\n",
    "    \"\"\"\n",
    "    Segments text into chunks from the specified file path, based on its extension.\n",
    "    \"\"\"\n",
    "    if file_path.lower().endswith('.txt'):\n",
    "        with open(file_path, 'r') as file:\n",
    "            full_text = file.read()\n",
    "\n",
    "        text_segmenter = RecursiveCharacterTextSplitter(\n",
    "            separators=[\"\\n\\n\", \"\\n\", ' ', ''],\n",
    "            chunk_size=800,\n",
    "            chunk_overlap=50,\n",
    "            length_function=len,\n",
    "            is_separator_regex=False\n",
    "        )\n",
    "        return text_segmenter.create_documents([full_text])\n",
    "\n",
    "    elif file_path.lower().endswith('.pdf'):\n",
    "        pdf_loader = PyPDFLoader(file_path)\n",
    "        documents = pdf_loader.load()\n",
    "        text_segmenter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200\n",
    "        )\n",
    "        chunks = []\n",
    "        for document in documents:\n",
    "            chunks.extend(text_segmenter.create_documents([document]))\n",
    "        return chunks\n",
    "\n",
    "    else:\n",
    "        print(\"Unsupported file format. Acceptable formats include .txt and .pdf.\")\n",
    "        return None\n",
    "\n",
    "def sanitize_text(raw_text):\n",
    "    \"\"\"\n",
    "    Cleans and standardizes the provided text string by removing unwanted characters\n",
    "    and fixing common formatting issues.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cleaned_text = re.sub(r'[^a-zA-Z0-9 .,;:\\'\"\\(\\)\\[\\]\\-]', '', raw_text)\n",
    "        cleaned_text = re.sub(r'(?<=[.,;:\\'\"])(?=[^\\s])', ' ', cleaned_text)\n",
    "        cleaned_text = re.sub(r'(?<=[^\\s])(?=[.,;:\\'\"])', ' ', cleaned_text)\n",
    "        cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "        cleaned_text = re.sub(r'(\\d+)([A-Za-z])', r'\\1 \\2', cleaned_text)\n",
    "        cleaned_text = re.sub(r'([A-Za-z])(\\d+)', r'\\1 \\2', cleaned_text)\n",
    "        cleaned_text = cleaned_text.strip()\n",
    "        return cleaned_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error while processing text: {e}\")\n",
    "        return raw_text\n",
    "\n",
    "def create_text_chunks(input_text):\n",
    "    \"\"\"\n",
    "    Creates manageable chunks of text from a larger input text string.\n",
    "    \"\"\"\n",
    "    segmenter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", ' ', ''],\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=50,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False\n",
    "    )\n",
    "    return [sanitize_text(chunk.page_content) for chunk in segmenter.create_documents([input_text])]\n",
    "\n",
    "def construct_dataframe(text):\n",
    "    \"\"\"\n",
    "    Constructs a dataframe from chunks of text, appending sentence embeddings.\n",
    "    \"\"\"\n",
    "    chunks = create_text_chunks(text)\n",
    "    df = pd.DataFrame(chunks, columns=[\"text\"])\n",
    "    embeddings = embedding_model.encode(df['text'].tolist(), normalize_embeddings=True)\n",
    "    df['text_vector'] = embeddings.tolist()\n",
    "    return df\n",
    "\n",
    "def generate_text_embedding(input_text):\n",
    "    \"\"\"\n",
    "    Generates a normalized embedding for a given text string.\n",
    "    \"\"\"\n",
    "    return embedding_model.encode(input_text, normalize_embeddings=True).tolist()\n",
    "\n",
    "def concatenate_vector_results(results):\n",
    "    \"\"\"\n",
    "    Concatenates a list of strings into a single string.\n",
    "    \"\"\"\n",
    "    return ' '.join(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578fa860",
   "metadata": {},
   "source": [
    "# LLM API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61e6f32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI, APIError, APIConnectionError, RateLimitError\n",
    "\n",
    "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def retrieve_response_from_llm(question, combined_vectors):\n",
    "    \"\"\"\n",
    "    Retrieves a response from the OpenAI Language Learning Model (LLM) based on the\n",
    "    provided question and combined vectors context.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        message_context = [\n",
    "            {'role': 'system', 'content': PERSONA},\n",
    "            {'role': 'user', 'content': str(combined_vectors)},\n",
    "            {'role': 'assistant', 'content': ASSISTANT_RESPONSE},\n",
    "            {'role': 'user', 'content': question}\n",
    "        ]\n",
    "        chat_completion = openai_client.chat.completions.create(\n",
    "            messages=message_context,\n",
    "            model=OPENAI_LLM,\n",
    "            temperature=0.6,\n",
    "        )\n",
    "        answer = chat_completion.choices[0].message.content\n",
    "        return answer\n",
    "\n",
    "    except APIConnectionError as e:\n",
    "        print(f\"Connection to OpenAI API failed: {e}\")\n",
    "        return \"Connection to OpenAI API failed.\"\n",
    "\n",
    "    except RateLimitError as e:\n",
    "        print(f\"Rate limit exceeded for OpenAI API requests: {e}\")\n",
    "        return \"Rate limit exceeded for OpenAI API requests.\"\n",
    "\n",
    "    except APIError as e:\n",
    "        print(f\"An error was returned by the OpenAI API: {e}\")\n",
    "        return \"An error was returned by the OpenAI API.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return \"An unexpected error occurred. Please try again later.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fec6c0",
   "metadata": {},
   "source": [
    "# Abstracted functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a52ebece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_database_and_insert_data(uploaded_file):\n",
    "    \"\"\"\n",
    "    Initializes the database by potentially dropping the existing table, creating a new table, and inserting\n",
    "    data extracted from the uploaded file.\n",
    "    \"\"\"\n",
    "    print(\"Attempting to drop existing table: \", remove_table())\n",
    "    print(\"Attempting to create a new table: \", create_data_table())\n",
    "    print(\"Current records in the table: \", count_table_records())\n",
    "    \n",
    "    text = extract_text_from_file_path(uploaded_file)\n",
    "    dataframe = construct_dataframe(text)\n",
    "    insertion_status = insert_table_data(dataframe)\n",
    "    print(\"Updated records in the table: \", count_table_records())\n",
    "    return insertion_status\n",
    "\n",
    "def generate_response_to_question(question):\n",
    "    \"\"\"\n",
    "    Generates a response to the given question using the OpenAI Language Learning Model after searching\n",
    "    for relevant vectors in the database.\n",
    "    \"\"\"\n",
    "    question_embeddings = generate_text_embedding(question)\n",
    "    search_results = perform_vector_search(question_embeddings)\n",
    "    combined_context = concatenate_vector_results(search_results)\n",
    "    response = retrieve_response_from_llm(question, combined_context)\n",
    "    return response, search_results[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6af5438c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./sample_data/large_document.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14dd5f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to drop existing table:  True\n",
      "Attempting to create a new table:  True\n",
      "Current records in the table:  0\n",
      "Updated records in the table:  503\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialize_database_and_insert_data(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7db763d",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer, matching_docs = generate_response_to_question(\"What is the recommended dose?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b46b7e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The recommended dose for Imatinib Teva varies by condition:\n",
      "\n",
      "1. For HESCEL (Hypereosinophilic Syndrome/Chronic Eosinophilic Leukemia), the recommended dose is 100 mg per day, which can be increased to 400 mg per day based on response and absence of adverse reactions.\n",
      "   \n",
      "2. For GIST (Gastrointestinal Stromal Tumors), the recommended dose is 400 mg per day for adults with unresectable and/or metastatic malignant GIST. There is limited data on the effects of dose increases from 400 mg to 600 mg or 800 mg in patients progressing at a lower dose.\n",
      "\n",
      "For other conditions mentioned, such as CML (Chronic Myeloid Leukemia) and Ph-positive ALL (Philadelphia chromosome-positive Acute Lymphoblastic Leukemia), specific doses were not detailed in your text and typically depend on factors like patient response and specific treatment protocols. Always follow the prescribing doctor's instructions and guidelines.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30062097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['For CML and GIST , your doctor may prescribe a higher or lower dose depending on how you respond to the treatment . If your daily dose is 800 mg (2 capsules) , you should take one capsule in the morning and a second capsule in the evening . If you are being treated for Ph-positive ALL : The starting dose is 600 mg to be taken as 1 capsule of 400 mg plus 2 capsules of 100 mg once a day . If you are being treated for MDSMPD : The starting dose is 400 mg , to be taken as 1 capsule once a day . If you are being treated for HESCEL : The starting dose is 100 mg , to be taken as 1 capsule of 100 mg once a day . Your doctor may decide to increase the dose to 400 mg , to be taken as 1 capsule of 400 mg once a day , depending on how you respond to treatment . If you are being treated for DFSP :',\n",
       " 'For CML and GIST , your doctor may prescribe a higher or lower dose depending on how you respond to the treatment . If your daily dose is 800 mg (8 tablets) , you should take 4 tablets in the morning and 4 tablets in the evening . If you are being treated for Ph-positive ALL : The starting dose is 600 mg to be taken as 6 tablets once a day . If you are being treated for MDSMPD : The starting dose is 400 mg , to be taken as 4 tablets once a day . If you are being treated for HESCEL : The starting dose is 100 mg , to be taken as 1 tablet once a day . Your doctor may decide to increase the dose to 400 mg , to be taken as 4 tablets once a day , depending on how you respond to treatment . If you are being treated for DFSP :',\n",
       " 'For CML and GIST , your doctor may prescribe a higher or lower dose depending on how you respond to the treatment . If your daily dose is 800 mg (2 tablets) , you should take one tablet in the morning and a second tablet in the evening . If you are being treated for Ph-positive ALL : The starting dose is 600 mg to be taken as 1 tablet of 400 mg plus 2 tablets of 100 mg or as 1 tablets of 400 mg once a day . If you are being treated for MDSMPD : The starting dose is 400 mg , to be taken as 1 tablet once a day . If you are being treated for HESCEL : The starting dose is 100 mg , to be taken as 1 tablet of 100 mg once a day . Your doctor may decide to increase the dose to 400 mg , to be taken as 1 tablet of 400 mg once a day , depending on how you respond to treatment .',\n",
       " 'For CML and GIST , your doctor may prescribe a higher or lower dose depending on how you respond to the treatment . If your daily dose is 800 mg (8 capsules) , you should take 4 capsules in the morning and 4 capsules in the evening . If you are being treated for Ph-positive ALL : The starting dose is 600 mg to be taken as 6 capsules once a day . If you are being treated for MDSMPD : The starting dose is 400 mg , to be taken as 4 capsules once a day . If you are being treated for HESCEL : The starting dose is 100 mg , to be taken as 1 capsule once a day . Your doctor may decide to increase the dose to 400 mg , to be taken as 4 capsules once a day , depending on how you respond to treatment . If you are being treated for DFSP :',\n",
       " 'VCR (1 . 5 mgm 2 day , IV) : days 1 , 8 , and 15 DAUN (45 mgm 2 day bolus , IV) : days 1 and 2 CPM (250 mgm 2 dose q 12 h x 4 doses , IV) : days 3 and 4 PEG-ASP (2500 IUnitsm 2 , IM) : day 4 G-CSF (5 gkg , SC) : days 5-14 or until ANC 1500 post nadirTriple IT therapy (age-adjusted) : days 1 and 15 DEX (6 mgm 2 day , PO) : days 1-7 and 15-21 Intensification block 1(9 weeks)Methotrexate (5 gm 2 over 24 hours , IV) : days 1 and 15 Leucovorin (75 mgm 2 at hour 36 , IV ; 15 mgm 2 IV or PO q 6 h x 6 doses)iii : days 2 , 3 , 16 , and 17 Triple IT therapy (age-adjusted) : days 1 and 22 VP-16 (100 mgm 2 day , IV) : days 22-26 CPM (300 mgm 2 day , IV) : days 22-26 MESNA (150 mgm 2 day , IV) : days 22-26 G-CSF (5 gkg , SC) : days 27-36 or until ANC 1500 post nadirARA-C (3 gm 2 , q 12 h , IV) : days 43 , 44']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matching_docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
